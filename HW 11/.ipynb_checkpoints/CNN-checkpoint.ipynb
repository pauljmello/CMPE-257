{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e01c5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paul-Jason Mello\n",
    "# Professor Shim\n",
    "# CMPE 257\n",
    "# May 5th, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc5e92",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32055d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e61f2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9a5b652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c53a84a",
   "metadata": {},
   "source": [
    "## 1. What are Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ed5a5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolution Neural Networks (CNNs) are neural networks which are explicity designed for image inputs. CNNs work by \n",
    "# creating a filter which is dragged over pixel clusters by a stride. As we navigate through the image we can create an\n",
    "# image which is better condensed and helps us extract meaningful information. CNNs are a supervised learning algorithms\n",
    "# which can be trained to do many things accurately. This versatility and accuracy makes them commonly used in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8832011",
   "metadata": {},
   "source": [
    "## 2. Why CNNs were introduced when Fully connected ANNs were already there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4e929b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANNs are better used for text data, while CNNs are used for image data. My understanding is that there are certain \n",
    "# properties that can be extracted from a CNN which ANN struggles too. Specifically, the filter we drag over the image\n",
    "# helps contain properties regarding the spatial relation between pixels. This may be highly complex data that needs a \n",
    "# strong architecture like CNN to extract such features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32169e18",
   "metadata": {},
   "source": [
    "## 3. What is meant by the following terms: convolutional layer, pooling layer, padding, stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05783e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Layer\n",
    "# \n",
    "# A convolutional layer is the NxM frame we slide over the image. This filter outputs a downsampled version of this data.\n",
    "# This can help to create a feature map which is capable of extracting complex data relationships.\n",
    "\n",
    "# Pooling Layer\n",
    "# \n",
    "# \n",
    "# A pooling layer is essentially when we take a filter and slide it over an image to compress and extract information. \n",
    "# We select the type of pooling we want to apply, such as max or average pooling, and then calculate the max or average\n",
    "# at each filter timestep. This downsamples the image but keeps important general properties that may be useful.\n",
    "# This can directly help reducing the dimensions of the data.\n",
    "\n",
    "# Padding\n",
    "# \n",
    "# The idea of padding is to create a buffer of unused data on the edges of an image. The zero-padding process helps to \n",
    "# prevent instances of image shrinking as we navigate through the consistantly small convolutional layers. It does this \n",
    "# by creating a frame of 0's around the image which acts as a buffer to reduce loss.  \n",
    "# \n",
    "# \n",
    "\n",
    "# Stride\n",
    "# \n",
    "# A stride is the pixel width x height that the filter is offset by when traversing the image. This may be one pixel to the\n",
    "# right at a time or we may skip the entire filter size. The goal depends on the purpose of conducting a swipe through the \n",
    "# image. In max pooling we would desire a stride which mimicked the filter size so downsampling would be 4 -> 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9efde8",
   "metadata": {},
   "source": [
    "## 4. What would be the size of the output if input is n^2, filter is f^2 and stride is of s "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0eff4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N^2 = N x N matrix\n",
    "# F^2 = F x F matrix\n",
    "# N = 32, F = 3, s = 1 \n",
    "# \n",
    "# ((N - F) / (s)) + 1 \n",
    "#\n",
    "# ((32 - 3) / 1) + 1 = 30, 30, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2777f3b",
   "metadata": {},
   "source": [
    "## 5. What are pre-trained models and what do you mean by transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa3d5f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained models are models which have an abstract understanding of a defined goal. They are models which can be used\n",
    "# as a starting point to build off of because the models themselves have been trained previously. This process of using\n",
    "# another model as the starting point has become known as transfer learning. In this way we can take patterns and \n",
    "# generalizations from one model and apply it to another model to build off of. Transfer learning is desireable because\n",
    "# the models are often trained on large datasets and can provide very strong foundations to build off of."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b20a28",
   "metadata": {},
   "source": [
    "## 6. Discuss CPU vs GPU vs TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24c59fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "# The central processing unit (CPU) is the logic center of the computer and is comrpised of the control unit and the\n",
    "# arthimatic logic unit.\n",
    "\n",
    "# GPU\n",
    "# The graphical processing unit (GPU) helps with fidelity and parallel processing. It is there to help the graphical \n",
    "# interfaces run efficently. \n",
    "\n",
    "# TPU\n",
    "# The tensor processing unit is specifically designed for deep learning using tensors. This is a very specific device \n",
    "# for a very specific task. However, the benefits are immense as the architecture is very efficent for tensorflow work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95101e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each of these devices play a specific role to complete a specific task. The TPU is notable because of its tensor\n",
    "# capabilities which are exceptionally useful for neural networks. While the GPU and CPU are fast, being able to directly\n",
    "# compute linear algebra on hardware vastly speeds up the process of learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078190a4",
   "metadata": {},
   "source": [
    "## 7. Perform CNN classification on citrus leaves dataset from tensorflow \n",
    "##     (try to achieve minimum 90% accuracy and above on the test set)\n",
    "##     Can be found using the link: https://www.tensorflow.org/datasets/catalog/citrus_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa038058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 608 images belonging to 2 classes.\n",
      "Found 151 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(validation_split = 0.2, rescale = 1/255.0)\n",
    "trainingData = train.flow_from_directory(\"/Users/GIGA/CMPE 257/HW 11/citrus\", \n",
    "                                         target_size = (256, 256), batch_size = 32, \n",
    "                                         class_mode = \"binary\", subset = \"training\")\n",
    "validationData = train.flow_from_directory(\"/Users/GIGA/CMPE 257/HW 11/citrus\", \n",
    "                                           target_size = (256, 256), batch_size = 32,\n",
    "                                           class_mode = \"binary\", subset = \"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c58b5fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        36896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 19365     \n",
      "=================================================================\n",
      "Total params: 69,093\n",
      "Trainable params: 69,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Conv2D(128, (3, 3), activation = \"linear\", input_shape = (256, 256, 3)),\n",
    "    layers.MaxPooling2D(pool_size = (4, 4), padding = 'same'),\n",
    "    layers.Conv2D(32, (3, 3), activation = \"relu\"),\n",
    "    layers.MaxPooling2D(pool_size = (5, 5), padding = 'same'),\n",
    "    layers.Conv2D(32, (3, 3), activation = \"relu\"),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(5, activation = \"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38c6800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"Adamax\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b1ea962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "19/19 [==============================] - 41s 2s/step - loss: 0.6135 - accuracy: 0.7747 - val_loss: 0.3332 - val_accuracy: 0.7969\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 38s 2s/step - loss: 0.2534 - accuracy: 0.8931 - val_loss: 0.1331 - val_accuracy: 0.9531\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 40s 2s/step - loss: 0.1225 - accuracy: 0.9638 - val_loss: 0.1031 - val_accuracy: 0.9453\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 38s 2s/step - loss: 0.0939 - accuracy: 0.9688 - val_loss: 0.0201 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 40s 2s/step - loss: 0.0659 - accuracy: 0.9770 - val_loss: 0.0891 - val_accuracy: 0.9609\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ae6fc24c10>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainingData, steps_per_epoch = trainingData.samples // 32, validation_data = validationData,  \n",
    "          validation_steps = validationData.samples // 32, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40eac93e",
   "metadata": {},
   "source": [
    "## 8. Plot the model architecture and explain how did you decide number of layers, filter size and other hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fae32ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 254, 254, 128)     3584      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        36896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 3872)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 19365     \n",
      "=================================================================\n",
      "Total params: 69,093\n",
      "Trainable params: 69,093\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f900bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this model, my aim was to continually condense the image through a rapid series of max pooling. I kept the layer\n",
    "# count low because initial testing demonstrated I didnt need many layers. The filter sizes were arbitrarily chosen to be\n",
    "# 3x3. Finally I decided to use a relu activation function because we are using image data. Overall I was able to achieve \n",
    "# 100% accuracy in only two epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5fb8b5",
   "metadata": {},
   "source": [
    "## 9. Increase the accuracy of the model in the demo file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a93cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done, increased from 91.9 -> 92.3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
